{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oDEIXBSjXL5e",
        "u0DP19b6XLot",
        "Zno7j6fmBz5L"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Raw code"
      ],
      "metadata": {
        "id": "HdbzJthTBXOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "import random\n",
        "import statistics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import arff\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "\n",
        "\n",
        "class MTR:\n",
        "  def __init__(self, model=None, X_train=None, X_test=None, y_train=None, y_test=None, feature_names=None, target_names=None):  \n",
        "    \"\"\"Init function\n",
        "        Args:\n",
        "            model: The trained RF model\n",
        "            trainData: the data that the RF was trained on\n",
        "            feature_names: The names of the features from our dataset\n",
        "            target_names: The names of the targets from our dataset\n",
        "            mae: the mean absolute error of the trained RF model\n",
        "            targets: the number of target values\n",
        "        Attributes:\n",
        "            model: The classifier/regression model\n",
        "            trees: The trees of an trained ensemble system\n",
        "            feature_names: The names of the features\n",
        "            min_max_feature_values: A helping dictionary for the path/feature reduction process\n",
        "            ranked_features: The features ranked based on SHAP Values (Small-Medium Datasets) or Feature Importance (Huge Datasets)\n",
        "    \"\"\"\n",
        "    if X_train is None or X_test is None or y_train is None or y_test is None:\n",
        "      print(\"non specified data\")\n",
        "    if feature_names is None:\n",
        "      print(\"non specified features names\")\n",
        "    if target_names is None:\n",
        "      print(\"non specified target names\")\n",
        "\n",
        "    self.trainData = X_train\n",
        "    self.X_test = X_test\n",
        "    self.y_train = y_train\n",
        "    self.y_test = y_test\n",
        "\n",
        "    if model is not None:\n",
        "      self.model = model\n",
        "    else:\n",
        "      # add gridsearch\n",
        "      parameters = [{\n",
        "         'criterion': ['squared_error'],#, 'absolute_error'],\n",
        "         #'max_depth': [2],#, 2, 5],   ----> if it does not extend fully, it may have an issue\n",
        "         'max_features': ['sqrt'],#, 'log2', 0.75, None],\n",
        "         'min_samples_leaf' : [1, 2, 5, 10]\n",
        "      }]\n",
        "      RF = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42)\n",
        "      clf = GridSearchCV(estimator=RF, param_grid=parameters, cv=10, n_jobs=-1, verbose=0, scoring='neg_mean_absolute_error')\n",
        "      clf.fit(self.trainData, self.y_train)\n",
        "      RF = clf.best_estimator_\n",
        "      self.model = RF\n",
        "    self.allowed_error = 0\n",
        "    self.amountOfReduction = None\n",
        "    self.trees = self.model.estimators_  # model is never None\n",
        "    self.predicted = self.model.predict(self.X_test)\n",
        "    self.feature_names = feature_names\n",
        "    self.target_names = target_names\n",
        "    if target_names is not None:\n",
        "      self.targets = len(target_names)\n",
        "    self.silly_local_importance = {} # It will fill in only if AR reduction will be applied!\n",
        "    self.min_max_feature_values = {}\n",
        "    self.ranked_features = {}\n",
        "    self.feature_rule_limits = {} # for testing\n",
        "    self.decisions_and_erros = [] # for testing\n",
        "    \n",
        "\n",
        "  def getModel(self):\n",
        "    if self.model is not None:\n",
        "      return self.model\n",
        "    print(\"you should define the model first\")\n",
        "\n",
        "  def getAllowedError(self):\n",
        "    return self.allowed_error\n",
        "\n",
        "  # for testing\n",
        "  def getFeatureLimits(self):\n",
        "    return self.feature_rule_limits\n",
        "\n",
        "  # for testing\n",
        "  def getDecisionsAndErros(self):\n",
        "    return self.decisions_and_erros\n",
        "\n",
        "  def getAmountOfReduction(self):\n",
        "    return self.amountOfReduction\n",
        "\n",
        "  def fitError(self, allowed_error=None):\n",
        "    # if error is int, create array of 1xself.targets with the same error\n",
        "    # if error is list of len=1, the final errors on the rule will average less than the error\n",
        "    # if error is list with len=self.targets, the final errors on the rule will be less than the respective error\n",
        "    if allowed_error is not None:\n",
        "      if type(allowed_error) == int:\n",
        "        self.allowed_error = np.array([allowed_error] * self.targets)\n",
        "      else:\n",
        "         self.allowed_error = np.array(allowed_error)\n",
        "    else:\n",
        "      self.allowed_error = mean_absolute_error(self.predicted, self.y_test, multioutput=\"raw_values\")\n",
        "\n",
        "  def explain(self, instance, allowed_error=None):\n",
        "    # fit the model\n",
        "    self.fitError(allowed_error)\n",
        "\n",
        "    rules, predictions = self.label_paths(instance) # ranges=rules\n",
        "\n",
        "    # find min/max of all leaves per tree\n",
        "    minmax = self.find_regression_trees_min_maxes(self.feature_names)\n",
        "\n",
        "    # reduce the rules\n",
        "    reduced_rules, reduced_probabilities, local_error = self._reduce_through_association_rules(rules, predictions)\n",
        "    self.amountOfReduction = [len(reduced_rules), len(rules)]\n",
        "\n",
        "    # compose the final rule\n",
        "    return self.composeRule(instance, reduced_rules, local_error)\n",
        "\n",
        "\n",
        "\n",
        "  def label_paths(self, instance):\n",
        "    \"\"\"label_paths function finds the ranges and predictions for each label\n",
        "    Args:\n",
        "        instance: The instance we want to find the paths\n",
        "    Return:\n",
        "        a list which contains a dictionary with features as keys and their min max ranges as values per tree\n",
        "        and a list with the predictions of each tree for the examined instance\n",
        "    \"\"\"\n",
        "    ranges = []\n",
        "    predictions = []\n",
        "    for tree in self.trees:\n",
        "      n_tree_prediction = []\n",
        "      for np in tree.predict([instance]):\n",
        "        n_tree_prediction.append(np) \n",
        "      tree_prediction = n_tree_prediction\n",
        "      path = tree.decision_path([instance])\n",
        "      leq = {}  # leq: less equal ex: x <= 1\n",
        "      b = {}  # b: bigger ex: x > 0.6\n",
        "      local_range = {}\n",
        "      for node in path.indices:\n",
        "        feature_id = tree.tree_.feature[node]\n",
        "        feature = self.feature_names[feature_id]\n",
        "        threshold = tree.tree_.threshold[node]\n",
        "        if threshold != -2.0:\n",
        "          if instance[feature_id] <= threshold:\n",
        "            leq.setdefault(feature, []).append(threshold)\n",
        "          else:\n",
        "            b.setdefault(feature, []).append(threshold)\n",
        "      for k in leq:\n",
        "        local_range.setdefault(k, []).append(['<=', min(leq[k])])  # !!\n",
        "      for k in b:\n",
        "        local_range.setdefault(k, []).append(['>', max(b[k])])  # !!\n",
        "      ranges.append(local_range)\n",
        "      predictions.append(list(tree_prediction[0]))\n",
        "    return ranges, predictions\n",
        "\n",
        "\n",
        "  def tree_to_code(self, tree, feature_names):\n",
        "    tree_ = tree.tree_\n",
        "    feature_name = [feature_names[i] for i in tree_.feature]\n",
        "    leaf_nodes = []\n",
        "    def recurse(node, depth):\n",
        "      indent = \"  \" * depth\n",
        "      if tree_.feature[node] != -2:\n",
        "        name = feature_name[node]\n",
        "        threshold = tree_.threshold[node]\n",
        "        temp = []\n",
        "        [temp.append(t) for t in recurse(tree_.children_left[node], depth + 1)]\n",
        "        [temp.append(t) for t in recurse(tree_.children_right[node], depth + 1)]\n",
        "        return temp\n",
        "      else:\n",
        "          return([node])\n",
        "\n",
        "    leaf_nodes.append(recurse(0, 1))\n",
        "    return(leaf_nodes[0])\n",
        "\n",
        "\n",
        "  # we want ths to find the minmax of excluded trees\n",
        "  def find_regression_trees_min_maxes(self, feature_names):\n",
        "    \"\"\" finds min max of the leaves for each tree\n",
        "    Args:\n",
        "        trees: list of estimators, the examined trees\n",
        "        num_of_targets: the number of the targets in the regression\n",
        "    Return:\n",
        "        A dict that contains a list of num_of_targets*2 values, first half are the min\n",
        "        of each target, last half are maxes.\n",
        "        example: [array([0.]), array([20.]), array([17.19]), | array([29.]), array([78.]), array([58.53])]\n",
        "    \"\"\"\n",
        "    trees = self.trees\n",
        "    min_max_leaf_prediction_per_tree = {}\n",
        "    for i in range(len(trees)):\n",
        "      tree = trees[i]\n",
        "      min_max_leaf_prediction_per_tree[i] = [None for i in range(self.targets*2)]\n",
        "      leaf_nodes = self.tree_to_code(tree, feature_names)\n",
        "      for l in leaf_nodes:\n",
        "        # here value returns the 3 targets per leaf and we want their minmax\n",
        "        value = tree.tree_.value[l]\n",
        "        for target in range(len(value)):\n",
        "          if min_max_leaf_prediction_per_tree[i][target+len(value)] is None or value[target] > min_max_leaf_prediction_per_tree[i][target+len(value)]:\n",
        "              min_max_leaf_prediction_per_tree[i][target+len(value)] = value[target]\n",
        "          if  min_max_leaf_prediction_per_tree[i][target] is None or value[target] < min_max_leaf_prediction_per_tree[i][target]:\n",
        "              min_max_leaf_prediction_per_tree[i][target] = value[target]\n",
        "    self.min_max_leaf_prediction_per_tree = min_max_leaf_prediction_per_tree\n",
        "    return min_max_leaf_prediction_per_tree\n",
        "\n",
        "\n",
        "\n",
        "  # Algorithm 6\n",
        "  #https://github.com/intelligence-csd-auth-gr/LionLearn/blob/1931ed50a2ca47f80243ce31e258d3f5fa9e701f/LionForests/lionforests.py#L1022\n",
        "  def _reduce_through_distribution_multi(self, instance, rules, predictions, instance_qe, method, targets):\n",
        "    \"\"\" path reduction\n",
        "    Args:\n",
        "        instance: used for the random seed\n",
        "        rules: we get them from func label_paths\n",
        "        predictions: predictions per tree, for the given target. we get them from func label_paths\n",
        "        instance_qe: allowed error\n",
        "        method: R2 for inner and R3 for outter\n",
        "        target: a list with index of targets\n",
        "    Return:\n",
        "        the reduced rules and reduced predictions for the predifined targets\n",
        "    \"\"\"\n",
        "    # the final results\n",
        "    local_error_per_target = []\n",
        "    reduced_rules_per_target = []\n",
        "    reduced_predictions_per_target = []\n",
        "\n",
        "    # loop for each selected target\n",
        "    for target in targets:\n",
        "      reduced_rules = rules\n",
        "      reduced_predictions = predictions[:,target]\n",
        "      real_prediction = np.array(predictions[:,target]).mean()\n",
        "      min_errors = abs(instance_qe)\n",
        "      min_s = 0\n",
        "\n",
        "      for s in [.1, .2, .5, 1, 2, 4, 5, 6, 7, 8, 9, 10, 20, 50, 100]:\n",
        "        np.random.seed(42)\n",
        "        normal_dis = np.random.normal(real_prediction, np.array(predictions[:,target]).std()/s, 100)\n",
        "        c = 0\n",
        "        l_reduced_rules = []\n",
        "        l_reduced_predictions = []\n",
        "\n",
        "\n",
        "        # for each tree basically\n",
        "        for i in predictions[:,target]:\n",
        "          # R2 is inner and R3 is outter\n",
        "          if (method == 'R2' and not (i < normal_dis.min() or i > normal_dis.max())) or (method == 'R3' and (i < normal_dis.min() or i > normal_dis.max())):\n",
        "              l_reduced_rules.append(rules[c])\n",
        "              l_reduced_predictions.append(predictions[:,target][c])\n",
        "          else:\n",
        "              # the min\n",
        "              dis_a = abs(i - self.min_max_leaf_prediction_per_tree[c][target][0])\n",
        "              # the max\n",
        "              dis_b = abs(i - self.min_max_leaf_prediction_per_tree[c][target+self.targets][0])\n",
        "\n",
        "              if dis_a < dis_b:\n",
        "                  l_reduced_predictions.append(self.min_max_leaf_prediction_per_tree[c][target+self.targets][0])\n",
        "              else:\n",
        "                  l_reduced_predictions.append(self.min_max_leaf_prediction_per_tree[c][target][0])\n",
        "          c = c + 1\n",
        "\n",
        "        l_error = abs(np.array(l_reduced_predictions).mean() - real_prediction)\n",
        "        if l_error < abs(instance_qe) and len(l_reduced_rules) < len(reduced_rules):\n",
        "            reduced_rules = l_reduced_rules\n",
        "            reduced_predictions = l_reduced_predictions\n",
        "            min_errors = l_error\n",
        "            min_s = s\n",
        "\n",
        "      local_error_per_target.append(l_error)\n",
        "      reduced_rules_per_target.append(reduced_rules)\n",
        "      reduced_predictions_per_target.append(reduced_predictions)\n",
        "    return local_error_per_target, reduced_rules_per_target, reduced_predictions_per_target\n",
        "\n",
        "\n",
        "  #https://github.com/intelligence-csd-auth-gr/LionLearn/blob/1931ed50a2ca47f80243ce31e258d3f5fa9e701f/LionForests/lionforests.py#L715\n",
        "  def _reduce_through_association_rules(self, rules, probabilities):\n",
        "    \"\"\" path reduction\n",
        "    Args:\n",
        "        rules: we get them from func label_paths\n",
        "        probabilities: predictions per tree, for the given target. we get them from func label_paths\n",
        "    Return:\n",
        "        reduced rules and the reduced predictions\n",
        "    \"\"\"\n",
        "    reduced_rules = rules\n",
        "    reduced_probabilities = probabilities\n",
        "\n",
        "    # create itemsets of features per rule create from estimators\n",
        "    get_itemsets = []\n",
        "    items = set()\n",
        "    # for each rule of the estimators\n",
        "    for rule in rules:\n",
        "      itemset = []\n",
        "      # for each feature of the rule\n",
        "      for p in rule:\n",
        "        itemset.append(p)\n",
        "        items.add(p)\n",
        "      get_itemsets.append(itemset)\n",
        "    max_number_of_features = len(items) # all distinct features\n",
        "    del items\n",
        "\n",
        "    # one-hot transform into boolean array and put in df\n",
        "    tEncoder = TransactionEncoder()\n",
        "    oneHotItemset = tEncoder.fit(get_itemsets).transform(get_itemsets)\n",
        "    df = pd.DataFrame(oneHotItemset, columns=tEncoder.columns_)\n",
        "    \n",
        "    # run association rules and get frequent itemsets (fi) (ADD fpgrowth)\n",
        "    temp_fi = apriori(df, min_support=0.1, use_colnames=True)\n",
        "    if len(temp_fi.values) == 0:\n",
        "        return rules, probabilities\n",
        "\n",
        "    # get the frequent itemsets \n",
        "    frequent_itemsets = (\n",
        "      association_rules(temp_fi, metric=\"support\", min_threshold=0.1)\n",
        "      .sort_values(by=\"confidence\",ascending=True)\n",
        "    )\n",
        "    \n",
        "    # Collect features and their importance from the association rules\n",
        "    probability = 0\n",
        "    k = 1\n",
        "    antecedents = []\n",
        "    antecedents_weights = {}\n",
        "    antecedents_set = set()\n",
        "    wcounter = 0\n",
        "    for antecedent in list(frequent_itemsets['antecedents']):\n",
        "      if tuple(antecedent) not in antecedents_set:\n",
        "        antecedents_set.add(tuple(antecedent))\n",
        "        for antecedent_i in list(antecedent):\n",
        "          if antecedent_i not in antecedents:\n",
        "            antecedents.append(antecedent_i)\n",
        "      for antecedent_i in list(antecedent):\n",
        "        wcounter = wcounter + 1\n",
        "        if antecedent_i not in antecedents_weights:\n",
        "          antecedents_weights[antecedent_i] = 1/wcounter\n",
        "        else:\n",
        "          antecedents_weights[antecedent_i] = antecedents_weights[antecedent_i] + 1/wcounter\n",
        "    self.silly_local_importance = antecedents_weights # dict {feature: importance}\n",
        "    size_of_ar = len(antecedents)\n",
        "\n",
        "    \n",
        "    items = set() # may be redundant since it was calculated/deleted previously\n",
        "    new_feature_list = []\n",
        "    for pr in reduced_rules:\n",
        "      for p in pr:\n",
        "        items.add(p)\n",
        "    new_feature_list = list(items)\n",
        "\n",
        "    reduced_rules = []\n",
        "    #reduced_probabilities = []\n",
        "    local_error = 2 * abs(self.allowed_error)\n",
        "    keep_pids = []\n",
        "\n",
        "    while np.any(local_error > abs(self.allowed_error)) and k <= size_of_ar:\n",
        "      feature_set = set()\n",
        "      for i in range(0, k):\n",
        "        feature_set.add(antecedents[i])\n",
        "\n",
        "      new_feature_list = list(feature_set)\n",
        "      redundant_features = [\n",
        "          i for i in self.feature_names if i not in new_feature_list]\n",
        "      reduced_rules = []\n",
        "      pid = 0\n",
        "      keep_pids = []\n",
        "      reduced_probabilities = []\n",
        "      \n",
        "      # for each rule, for each target\n",
        "      for rule in rules:\n",
        "        reduced_probabilities_per_target = []\n",
        "        flag = True\n",
        "        for target in range(self.targets):\n",
        "          # in case of no redundant features in the rule\n",
        "          if sum([1 for j in redundant_features if j in rule]) == 0:\n",
        "            if flag: # this so the rule is added once per target\n",
        "              reduced_rules.append(rule) # will get this using keep_pids\n",
        "              flag=False\n",
        "            reduced_probabilities_per_target.append(probabilities[pid][target])\n",
        "            keep_pids.append(pid)\n",
        "          else:\n",
        "            dis_a = abs(probabilities[pid][target]- self.min_max_leaf_prediction_per_tree[pid][target][0])\n",
        "            dis_b = abs(probabilities[pid][target] - self.min_max_leaf_prediction_per_tree[pid][target+self.targets][0])\n",
        "            if dis_a < dis_b:\n",
        "              reduced_probabilities_per_target.append(self.min_max_leaf_prediction_per_tree[pid][target+self.targets][0])\n",
        "            else:\n",
        "              reduced_probabilities_per_target.append(self.min_max_leaf_prediction_per_tree[pid][target][0])\n",
        "        pid = pid + 1\n",
        "        reduced_probabilities.append(reduced_probabilities_per_target)\n",
        "      local_error = mean_absolute_error(probabilities, reduced_probabilities, multioutput=\"raw_values\")\n",
        "      k += 1\n",
        "\n",
        "    # reset\n",
        "    if np.any(local_error > abs(self.allowed_error)) and k > size_of_ar:  \n",
        "      keep_pids = []\n",
        "      reduced_rules = []\n",
        "      reduced_probabilities = []\n",
        "      pid = 0\n",
        "      for i in rules:\n",
        "        reduced_rules.append(i)\n",
        "        reduced_probabilities.append(probabilities[pid])\n",
        "        keep_pids.append(pid)\n",
        "        pid = pid + 1\n",
        "    temp_pids = keep_pids.copy()\n",
        "    last_pid = None\n",
        "\n",
        "    while np.all(local_error < abs(self.allowed_error)) and len(temp_pids) > 2:\n",
        "      reduced_rules = []\n",
        "      pid = 0\n",
        "      reduced_probabilities = []\n",
        "      last_pid = temp_pids[-1]\n",
        "      temp_pids = temp_pids[:-1]\n",
        "\n",
        "      for rule in rules:\n",
        "        reduced_probabilities_per_target = []\n",
        "        if pid in temp_pids:\n",
        "          reduced_rules.append(rule)\n",
        "          for target in range(self.targets): \n",
        "            reduced_probabilities_per_target.append(probabilities[pid][target])\n",
        "        else:\n",
        "          for target in range(self.targets):\n",
        "            dis_a = abs(probabilities[pid][target] - self.min_max_leaf_prediction_per_tree[pid][target][0])\n",
        "            dis_b = abs(probabilities[pid][target] - self.min_max_leaf_prediction_per_tree[pid][target+self.targets][0])\n",
        "            if dis_a < dis_b:\n",
        "                reduced_probabilities_per_target.append(self.min_max_leaf_prediction_per_tree[pid][target+self.targets][0])\n",
        "            else:\n",
        "                reduced_probabilities_per_target.append(self.min_max_leaf_prediction_per_tree[pid][target][0])\n",
        "        pid = pid + 1\n",
        "        reduced_probabilities.append(reduced_probabilities_per_target)\n",
        "\n",
        "      local_error = mean_absolute_error(probabilities, reduced_probabilities, multioutput=\"raw_values\")\n",
        "\n",
        "    if last_pid is not None:\n",
        "      temp_pids.append(last_pid)\n",
        "      reduced_rules = []\n",
        "      pid = 0\n",
        "      reduced_probabilities = []\n",
        "\n",
        "      for rule in rules:\n",
        "        reduced_probabilities_per_target = []\n",
        "        if pid in temp_pids:\n",
        "          reduced_rules.append(rule)\n",
        "          for target in range(self.targets): \n",
        "            reduced_probabilities_per_target.append(probabilities[pid][target])\n",
        "        else:\n",
        "          for target in range(self.targets):\n",
        "            dis_a = abs(probabilities[pid][target] - self.min_max_leaf_prediction_per_tree[pid][target][0])\n",
        "            dis_b = abs(probabilities[pid][target] - self.min_max_leaf_prediction_per_tree[pid][target+self.targets][0])\n",
        "            if dis_a < dis_b:\n",
        "                reduced_probabilities_per_target.append(self.min_max_leaf_prediction_per_tree[pid][target+self.targets][0])\n",
        "            else:\n",
        "                reduced_probabilities_per_target.append(self.min_max_leaf_prediction_per_tree[pid][target][0])\n",
        "        pid = pid + 1\n",
        "        reduced_probabilities.append(reduced_probabilities_per_target)\n",
        "\n",
        "      local_error = mean_absolute_error(probabilities, reduced_probabilities, multioutput=\"raw_values\")\n",
        "    local_error = mean_absolute_error(probabilities, reduced_probabilities, multioutput=\"raw_values\")\n",
        "    return reduced_rules, reduced_probabilities, local_error\n",
        "\n",
        "\n",
        "  def _pre_feature_range_caluclation(self, rules, feature):\n",
        "    ''' function that return the min and max values that a feature from the \n",
        "        reduced rules can get\n",
        "      args:\n",
        "        rules: the rules we got after the reduction\n",
        "        features: a particular feature out of the reduced feature set\n",
        "    '''\n",
        "    for i in range(len(self.feature_names)):\n",
        "      self.min_max_feature_values[self.feature_names[i]] = [min(self.trainData[:, i]), max(self.trainData[:, i])]\n",
        "\n",
        "    mi = None\n",
        "    ma = None\n",
        "    for i in rules:\n",
        "      if feature in i:\n",
        "        if len(i[feature]) == 1:\n",
        "          if i[feature][0][0] == \"<=\":\n",
        "            if ma is None or ma >= i[feature][0][1]:\n",
        "              ma = i[feature][0][1]\n",
        "          else:\n",
        "            if mi == None or mi <= i[feature][0][1]:\n",
        "              mi = i[feature][0][1]\n",
        "        else:\n",
        "          if mi == None or mi <= i[feature][1][1]:\n",
        "            mi = i[feature][1][1]\n",
        "          if ma == None or ma >= i[feature][0][1]:\n",
        "            ma = i[feature][0][1]\n",
        "    if mi is None:\n",
        "      mi = self.min_max_feature_values[feature][0]\n",
        "    if ma is None:\n",
        "      ma = self.min_max_feature_values[feature][1]\n",
        "    return [mi, ma]\n",
        "\n",
        "\n",
        "  # https://github.com/intelligence-csd-auth-gr/LionLearn/blob/1931ed50a2ca47f80243ce31e258d3f5fa9e701f/LionForests/lionforests.py#L522\n",
        "  def composeRule(self, instance, reduced_rules, local_error):\n",
        "    ''' function used to compose the final rule\n",
        "    '''\n",
        "    rule = \"if \"\n",
        "    temp_f_mins = {}\n",
        "    temp_f_maxs = {}\n",
        "    self.feature_rule_limits = {}\n",
        "    self.decisions_and_erros = []\n",
        "\n",
        "    # get the features that appear on the reduced rules\n",
        "    items = set()\n",
        "    for r in reduced_rules:\n",
        "      for feature in r:\n",
        "        items.add(feature)\n",
        "    local_feature_names = list(items)\n",
        "\n",
        "\n",
        "    for feature in self.feature_names:\n",
        "      if feature in local_feature_names:\n",
        "        mi, ma = self._pre_feature_range_caluclation(reduced_rules, feature)\n",
        "        temp_f_mins[feature] = mi\n",
        "        temp_f_maxs[feature] = ma\n",
        "\n",
        "    f_mins = []\n",
        "    f_maxs = []\n",
        "    for feature in self.feature_names:\n",
        "      if feature in temp_f_mins:\n",
        "        f_mins.append(temp_f_mins[feature])\n",
        "      else:\n",
        "        f_mins.append(0)\n",
        "      if feature in temp_f_maxs:\n",
        "        f_maxs.append(temp_f_maxs[feature])\n",
        "      else:\n",
        "        f_maxs.append(0)\n",
        "\n",
        "    # create the decision for all target values\n",
        "    decision = {}\n",
        "    pred = self.model.predict([instance])[0]\n",
        "    if local_error is not None:\n",
        "      for tar in range(len(self.target_names)):\n",
        "        decision[self.target_names[tar]] = self.target_names[tar] + ': ' + str(round(pred[tar], 4)) + \" +/- \" + str(round(local_error[tar], 4)) + \" error\"\n",
        "        self.decisions_and_erros.append([pred[tar], local_error[tar]])\n",
        "    else:\n",
        "      for tar in range(len(self.target_names)):\n",
        "        decision[self.target_names[tar]] = self.target_names[tar] + ': ' + str(round(pred[tar], 4))\n",
        "        self.decisions_and_erros.append([pred[tar], 0])\n",
        "    # we only use this for reference on the ranked features below, its the same for all targets\n",
        "    target_name = self.target_names[0]\n",
        "\n",
        "\n",
        "    d = {'Feature': self.feature_names,\n",
        "          'Importance': self.model.feature_importances_}\n",
        "    for ind in range(len(self.target_names)):\n",
        "      self.ranked_features[self.target_names[ind]] = \\\n",
        "          pd.DataFrame(data=d).sort_values(\n",
        "              by=['Importance'], ascending=False)['Feature'].values\n",
        "\n",
        "    # create the rule containing mins and maxes of the reduced features\n",
        "    for ranked_f in self.ranked_features[target_name]:\n",
        "      f = self.feature_names.get_loc(ranked_f)\n",
        "      if self.feature_names[f] in local_feature_names:\n",
        "        mmi = np.array([f_mins, f_mins])[0][f]\n",
        "        mma = np.array([f_maxs, f_maxs])[0][f]  # ena tab mesa\n",
        "        self.feature_rule_limits[self.feature_names[f]] = [mmi, mma]\n",
        "        rule = rule + str(round(mmi, 3)) + \"<=\" + self.feature_names[f] + \"<=\" + str(round(mma, 3)) + \" & \"\n",
        "\n",
        "    rule = rule[:-3] + \" then \"\n",
        "    for key in decision.keys():\n",
        "      rule += decision[key] + \", \"\n",
        "    return rule[:-2]\n"
      ],
      "metadata": {
        "id": "x9ih8ZSi_xlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING"
      ],
      "metadata": {
        "id": "RyPHy3GgBt0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Marlena.zip"
      ],
      "metadata": {
        "id": "zNgno-sqHcUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from XMTR import MTR\n",
        "from GlobalLocalVariants import GlobalSurrogateTree, LocalSurrogateTree\n",
        "from Marlena.algorithms.MARLENA.marlena.marlena.marlena import MARLENA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import arff\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore')\n",
        "import time"
      ],
      "metadata": {
        "id": "VwK-CgWxCvGy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_al_error(instance, perc):\n",
        "  # the error should be non zero\n",
        "  return (instance+0.1)*perc \n",
        "\n",
        "\n",
        "def rule_cov(instance, feature_names, rule):\n",
        "  covered = True\n",
        "  for k in range(len(instance)):\n",
        "    feature = feature_names[k]\n",
        "    if feature in rule.keys():\n",
        "      if type(rule[feature][0]) == list: # for GS/LS\n",
        "        for lst in rule[feature]:\n",
        "          if lst[0] == '>' and instance[k] <= lst[1]:\n",
        "            return 0\n",
        "          if lst[0] == '<=' and instance[k] > lst[1]:\n",
        "            return 0\n",
        "      else: # if it comes from MTR\n",
        "          if instance[k] > rule[feature][1]:  # 1=max\n",
        "              return 0\n",
        "          if instance[k] < rule[feature][0]:  # 0=min\n",
        "              return 0\n",
        "  return 1\n",
        "\n",
        "\n",
        "def calcMae(actualPred, MTRpred, GSpred, LSpred, MARLENApreds):\n",
        "  # mae MTR local error\n",
        "  MTRerrors = np.array([subarray[:,1] for subarray in MTRpred])\n",
        "  MTRpreds = np.array([subarray[:,0] for subarray in MTRpred])\n",
        "  column_errors = np.mean(MTRerrors, axis=0)\n",
        "  maeActual_with_error = np.mean(MTRerrors, axis=0)\n",
        "\n",
        "  # mae MTR/GS without local error\n",
        "  maeGS = mean_absolute_error(GSpred, MTRpreds, multioutput=\"raw_values\")\n",
        "\n",
        "  # mae MTR/LS without local error\n",
        "  maeLS = mean_absolute_error(LSpred, MTRpreds, multioutput=\"raw_values\")\n",
        "\n",
        "  # mae MTR/MARLENA without local error\n",
        "  maeMAR = mean_absolute_error(MARLENApreds, MTRpreds, multioutput=\"raw_values\")\n",
        "\n",
        "  return [maeActual_with_error, maeGS, maeLS, maeMAR]\n",
        "\n",
        "def doTest(X_train, X_test, y_train, y_test, f_n, t_n, percentage): \n",
        "  # train models\n",
        "  print('training MTR...') \n",
        "  MTR_obj = MTR(model=None, X_train = X_train, X_test=X_test, y_train=y_train, y_test=y_test, feature_names=f_n, target_names=t_n)\n",
        "  model = MTR_obj.getModel()\n",
        "  predictions = model.predict(X_train)\n",
        "\n",
        "  print('training GS...') \n",
        "  GS = GlobalSurrogateTree(X_train, predictions, f_n)\n",
        "  print('training LS...') \n",
        "  LS = LocalSurrogateTree(X_train, predictions, f_n, 20) # neigns should be >= 10\n",
        "  print('training Marlena...')\n",
        "  marlena = MARLENA(neigh_type='mixed', random_state=42)\n",
        "\n",
        "\n",
        "  actualpreds = []\n",
        "  MTRpreds = []\n",
        "  GSpreds = []\n",
        "  LSpreds = []\n",
        "  MARLENApreds = []\n",
        "\n",
        "  time_response = {'MTR': [], 'GS': [], 'LS': [], 'MARLENA': []}\n",
        "  avgEstimators = []\n",
        "  coverage = np.array([0,0,0,0])\n",
        "  avgRuleLengths = np.array([0,0,0,0])\n",
        "  for i in range(len(X_test)):\n",
        "    print(i+1,\"/\", len(X_test), \"tests\")\n",
        "    inside_coverage = np.array([0,0,0,0])\n",
        "    instance = X_test[i]\n",
        "    # actual\n",
        "    actualpreds.append(y_test[i])\n",
        "\n",
        "    # MTR\n",
        "    error = calc_al_error(y_test[i], percentage)\n",
        "    ts = time.time()\n",
        "    MTRrule = MTR_obj.explain(instance, error) # explain instance\n",
        "    te = time.time() - ts\n",
        "    time_response['MTR'].append(te)\n",
        "    estimators = MTR_obj.getAmountOfReduction() # get estimators\n",
        "    avgEstimators.append(estimators[0])\n",
        "    decisionsAndErrors = MTR_obj.getDecisionsAndErros() # get preds/errors\n",
        "    MTRpreds.append(decisionsAndErrors)\n",
        "    feature_limits = MTR_obj.getFeatureLimits()\n",
        "    avgRuleLengths[0] += len(feature_limits.keys())\n",
        "\n",
        "    # GS \n",
        "    ts = time.time()\n",
        "    GSrule, GSprediction = GS.rule(instance)\n",
        "    te = time.time() - ts\n",
        "    time_response['GS'].append(te)\n",
        "    GSpreds.append(GSprediction)\n",
        "    avgRuleLengths[1] += len(GSrule.keys())\n",
        "\n",
        "    # LS\n",
        "    ts = time.time()\n",
        "    LSrule, LSprediction = LS.rule(instance)\n",
        "    te = time.time() - ts\n",
        "    time_response['LS'].append(te)\n",
        "    LSpreds.append(LSprediction)\n",
        "    avgRuleLengths[2] += len(LSrule.keys())\n",
        "\n",
        "    # MARLENA\n",
        "    i2e = pd.Series(instance, index=f_n)\n",
        "    X2E = pd.DataFrame(X_train, columns=f_n)\n",
        "    ts = time.time()\n",
        "    # returns rule, mask(MarlenaPrediction), list_split_conditions, len_rule, instance_imporant_feat, fidelity, hit, DT\n",
        "    _, MarlenaPrediction, list_split_conditions, len_rule, _, _, _, _ = marlena.extract_explanation(i2e, X2E, model, f_n, [],\n",
        "                                              t_n, k=10, size=50, alpha=0.7)\n",
        "    te = time.time() - ts\n",
        "    time_response['MARLENA'].append(te)\n",
        "    MARLENApreds.append(MarlenaPrediction)\n",
        "    avgRuleLengths[3] += len_rule #len(list_split_conditions.keys())\n",
        "\n",
        "    # calculate the coverage\n",
        "    for test_instance in X_test:\n",
        "      MTRcov = rule_cov(test_instance, f_n, feature_limits)\n",
        "      GScov = rule_cov(test_instance, f_n, GSrule) \n",
        "      LScov = rule_cov(test_instance, f_n, LSrule) \n",
        "      MARcov = rule_cov(test_instance, f_n, list_split_conditions) \n",
        "      inside_coverage[0] += MTRcov\n",
        "      inside_coverage[1] += GScov\n",
        "      inside_coverage[2] += LScov\n",
        "      inside_coverage[3] += MARcov\n",
        "    coverage = np.add(coverage, inside_coverage/len(X_test))\n",
        "\n",
        "  actualpreds = np.array(actualpreds)\n",
        "  MTRpreds = np.array(MTRpreds)\n",
        "  GSpreds = np.array(GSpreds)\n",
        "  LSpreds = np.array(LSpreds)\n",
        "  MARLENApreds = np.array(MARLENApreds)\n",
        "\n",
        "  coverage = coverage/len(X_test)\n",
        "\n",
        "  avgRuleLengths = avgRuleLengths/len(X_test)\n",
        "  maeResults = calcMae(actualpreds, MTRpreds, GSpreds, LSpreds, MARLENApreds)\n",
        "\n",
        "  print('MTR|', \"mae: \", maeResults[0].round(3), \"| ruleL:\", avgRuleLengths[0], \"| TIME:\", np.array(time_response['MTR']).mean(), \"| Coverage:\",coverage[0], \"| avg estimators:\", round(np.array(avgEstimators).mean(),3),\"/\",estimators[1])\n",
        "  print(' GS|', \"mae: \", maeResults[1].round(3), \"| ruleL:\", avgRuleLengths[1], \"| TIME:\", np.array(time_response['GS']).mean(), \"| Coverage:\",coverage[1])\n",
        "  print(' LS|', \"mae: \", maeResults[2].round(3), \"| ruleL:\", avgRuleLengths[2], \"| TIME:\", np.array(time_response['LS']).mean(), \"| Coverage:\",coverage[2])\n",
        "  print('MAR|', \"mae: \", maeResults[3].round(3), \"| ruleL:\", avgRuleLengths[3], \"| TIME:\", np.array(time_response['MARLENA']).mean(), \"| Coverage:\",coverage[3])\n"
      ],
      "metadata": {
        "id": "Wulp_TlYO8zV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# slump"
      ],
      "metadata": {
        "id": "_mm1YfhG8rzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############\n",
        "# slump data 103x10, 3 of them targets\n",
        "############\n",
        "\n",
        "# load data\n",
        "slump_data = arff.loadarff('slump.arff')\n",
        "slump_df = pd.DataFrame(slump_data[0])\n",
        "\n",
        "# get column names\n",
        "column_names = slump_df.columns\n",
        "\n",
        "# get data/target names\n",
        "f_n = column_names[:7]\n",
        "t_n = column_names[7:]\n",
        "\n",
        "X = slump_df[f_n]\n",
        "y = slump_df[t_n]\n",
        "\n",
        "# convert to numpy\n",
        "X = X.to_numpy()\n",
        "y = y.to_numpy()\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "counter = 1\n",
        "for train_index, test_index in kf.split(X):\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "  print(\"iteration:\", counter)\n",
        "  counter += 1\n",
        "  doTest(X_train, X_test, y_train, y_test, f_n, t_n, 0.1)\n",
        "  break"
      ],
      "metadata": {
        "id": "V9002zcLPx04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b4cd8a-db66-4e0b-9bb7-9bfb5c3c3827"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration: 1\n",
            "training MTR...\n",
            "training GS...\n",
            "training LS...\n",
            "training Marlena...\n",
            "1 / 11 tests\n",
            "2 / 11 tests\n",
            "3 / 11 tests\n",
            "4 / 11 tests\n",
            "5 / 11 tests\n",
            "6 / 11 tests\n",
            "7 / 11 tests\n",
            "8 / 11 tests\n",
            "9 / 11 tests\n",
            "10 / 11 tests\n",
            "11 / 11 tests\n",
            "MTR| mae:  [1.774 3.235 1.909] | ruleL: 7.0 | TIME: 0.2058915441686457 | Coverage: 0.08264462809917357 | avg estimators: 92.273 / 100\n",
            " GS| mae:  [3.49  7.423 3.532] | ruleL: 4.0 | TIME: 0.0004927678541703658 | Coverage: 0.2231404958677686\n",
            " LS| mae:  [2.525 7.35  2.532] | ruleL: 2.4545454545454546 | TIME: 2.3376534418626265 | Coverage: 0.2809917355371901\n",
            "MAR| mae:  [2.161 5.714 1.988] | ruleL: 4.636363636363637 | TIME: 0.12489815191789107 | Coverage: 0.09090909090909093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# andro"
      ],
      "metadata": {
        "id": "FC0ATmtZRjYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############\n",
        "# andro data 49x30, 6 targets\n",
        "############\n",
        "\n",
        "# load data\n",
        "andro_data = arff.loadarff('andro.arff')\n",
        "andro_df = pd.DataFrame(andro_data[0])\n",
        "\n",
        "# get column names\n",
        "column_names = andro_df.columns\n",
        "\n",
        "# get data/target names\n",
        "f_n = column_names[:30]\n",
        "t_n = column_names[30:]\n",
        "\n",
        "X = andro_df[f_n]\n",
        "y = andro_df[t_n]\n",
        "\n",
        "# convert to numpy\n",
        "X = X.to_numpy()\n",
        "y = y.to_numpy()\n",
        "\n",
        "kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
        "counter = 1\n",
        "for train_index, test_index in kf.split(X):\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "  print(\"iteration:\", counter)\n",
        "  counter += 1\n",
        "  doTest(X_train, X_test, y_train, y_test, f_n, t_n, 100)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z993BmywRwaK",
        "outputId": "f1f09a15-7f5f-4cef-d925-0ee50cdc2710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration: 1\n",
            "training MTR...\n",
            "training GS...\n",
            "training LS...\n",
            "training Marlena...\n",
            "1 / 25 tests\n",
            "2 / 25 tests\n",
            "3 / 25 tests\n",
            "4 / 25 tests\n",
            "5 / 25 tests\n",
            "6 / 25 tests\n",
            "7 / 25 tests\n",
            "8 / 25 tests\n",
            "9 / 25 tests\n",
            "10 / 25 tests\n",
            "11 / 25 tests\n",
            "12 / 25 tests\n",
            "13 / 25 tests\n",
            "14 / 25 tests\n",
            "15 / 25 tests\n",
            "16 / 25 tests\n",
            "17 / 25 tests\n",
            "18 / 25 tests\n",
            "19 / 25 tests\n",
            "20 / 25 tests\n",
            "21 / 25 tests\n",
            "22 / 25 tests\n",
            "23 / 25 tests\n",
            "24 / 25 tests\n",
            "25 / 25 tests\n",
            "MTR| mae:  [0. 0. 0. 0. 0. 0.] | ruleL: 28.88 | TIME: 0.1266466522216797 | Coverage: 0.0192 | avg estimators: 100.0 / 100\n",
            " GS| mae:  [0.26  0.052 0.195 0.136 1.874 0.133] | ruleL: 3.48 | TIME: 0.000635986328125 | Coverage: 0.17120000000000007\n",
            " LS| mae:  [0.282 0.054 0.223 0.166 2.245 0.156] | ruleL: 3.96 | TIME: 3.412086591720581 | Coverage: 0.16160000000000005\n",
            "MAR| mae:  [0.3   0.064 0.253 0.185 2.725 0.177] | ruleL: 5.84 | TIME: 0.14638819694519042 | Coverage: 0.09920000000000004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# osales NaN values"
      ],
      "metadata": {
        "id": "oDEIXBSjXL5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ############\n",
        "# # osales 639x413, 12 targets\n",
        "# ############\n",
        "\n",
        "# # load data\n",
        "# osales_data = arff.loadarff('osales.arff')\n",
        "# osales_df = pd.DataFrame(osales_data[0])\n",
        "\n",
        "# # get column names\n",
        "# column_names = osales_df.columns\n",
        "\n",
        "# # get data/target names\n",
        "# f_n = column_names[:30]\n",
        "# t_n = column_names[30:]\n",
        "\n",
        "# X = osales_df[f_n]\n",
        "# y = osales_df[t_n]\n",
        "\n",
        "# # convert to numpy\n",
        "# X = X.to_numpy()\n",
        "# y = y.to_numpy()\n",
        "\n",
        "# kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
        "# counter = 1\n",
        "# for train_index, test_index in kf.split(X):\n",
        "#   X_train, X_test = X[train_index], X[test_index]\n",
        "#   y_train, y_test = y[train_index], y[test_index]\n",
        "#   print(\"iteration:\", counter)\n",
        "#   counter += 1\n",
        "#   doTest(X_train, X_test, y_train, y_test, f_n, t_n, 100)\n",
        "#   break"
      ],
      "metadata": {
        "id": "syI2DCs7XOGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# wq"
      ],
      "metadata": {
        "id": "vQNcHFD1VyuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############\n",
        "# wq 1060x16, 14 targets\n",
        "############\n",
        "\n",
        "# load data\n",
        "wq_data = arff.loadarff('wq.arff')\n",
        "wq_df = pd.DataFrame(wq_data[0])\n",
        "\n",
        "# get column names\n",
        "column_names = wq_df.columns\n",
        "\n",
        "# get data/target names\n",
        "f_n = column_names[:5]\n",
        "t_n = column_names[20:]\n",
        "\n",
        "X = wq_df[f_n]\n",
        "y = wq_df[t_n]\n",
        "\n",
        "# convert to numpy\n",
        "X = X.to_numpy()\n",
        "y = y.to_numpy()\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=25, shuffle=True, random_state=42)\n",
        "counter = 1\n",
        "for train_index, test_index in kf.split(X):\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "  print(\"iteration:\", counter)\n",
        "  counter += 1\n",
        "  doTest(X_train, X_test, y_train, y_test, f_n, t_n, 2)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7f7BDJ3V1pC",
        "outputId": "a907d601-94be-4887-cfa7-8517ea6c5854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration: 1\n",
            "training MTR...\n",
            "training GS...\n",
            "training LS...\n",
            "training Marlena...\n",
            "1 / 43 tests\n",
            "local_error [0.10701305 0.18176407 0.08481727 0.11113636 0.16815476 0.12390693\n",
            " 0.11658658 0.09795887 0.1397619  0.19051948]\n",
            "[[0.7220494301450185, 0.10701305067481538], [1.755568076187659, 0.18176406926406927], [0.4749187347310414, 0.08481726606726607], [1.3075536992677086, 0.11113636363636363], [2.399164564580586, 0.1681547619047619], [0.9356882673569981, 0.12390692640692641], [0.9865672783099251, 0.1165865800865801], [0.20846280400885667, 0.09795887445887445], [0.7477743061032532, 0.13976190476190475], [1.0528673302961225, 0.19051948051948053]]\n",
            "2 / 43 tests\n",
            "local_error [0.09019444 0.15888889 0.07407051 0.11116667 0.18       0.12458333\n",
            " 0.08890152 0.06938889 0.10954365 0.16693182]\n",
            "[[0.7232313691210751, 0.09019444444444444], [1.490709826937768, 0.15888888888888889], [0.18308638093932214, 0.07407051282051282], [0.1939048631107454, 0.11116666666666668], [0.7976246857717447, 0.18], [1.3349654406051463, 0.12458333333333334], [1.2429659547968372, 0.08890151515151516], [0.34616149944826413, 0.06938888888888889], [0.8763158859114736, 0.10954365079365079], [0.16884967809967807, 0.16693181818181815]]\n",
            "3 / 43 tests\n",
            "local_error [0.101298   0.17976281 0.09650107 0.14964015 0.19381854 0.15849026\n",
            " 0.12575595 0.09826551 0.14395563 0.19007846]\n",
            "[[1.1420034157019456, 0.10129800183476656], [1.684990350335939, 0.17976280663780664], [0.1925696941946941, 0.09650106837606838], [0.39342986996663465, 0.1496401515151515], [1.1468407246348422, 0.19381854256854258], [0.5769357064504126, 0.15849025974025974], [0.8182577006327008, 0.12575595238095238], [0.3442123219917338, 0.09826551226551228], [0.41033112802230454, 0.1439556277056277], [0.2993199716949717, 0.19007846320346322]]\n",
            "4 / 43 tests\n",
            "local_error [0.13674404 0.16898167 0.10271267 0.12653049 0.196481   0.15663686\n",
            " 0.11157262 0.08000129 0.12981035 0.21374775]\n",
            "[[0.5891106450738802, 0.13674403864109747], [1.4110798172415826, 0.16898166702578468], [0.2414060286119111, 0.10271266968325792], [0.39810454904572556, 0.12653048911872442], [1.474019012033718, 0.19648100265747323], [2.656152630049688, 0.156636859872154], [1.5649174363218479, 0.1115726169255581], [0.6919621269253622, 0.08000129282482224], [1.1731071469380296, 0.12981035304564714], [0.30255369793605086, 0.21374775224775225]]\n",
            "5 / 43 tests\n",
            "local_error [0.11183245 0.16554293 0.09683566 0.11890909 0.16719192 0.17692857\n",
            " 0.12056061 0.10188889 0.13272619 0.19721717]\n",
            "[[0.8186690181163866, 0.1118324534942182], [1.1924450286555552, 0.16554292929292927], [0.22525620432199375, 0.09683566433566433], [0.9531443366574943, 0.11890909090909091], [1.5655790729738095, 0.16719191919191922], [0.5179709296551402, 0.17692857142857144], [1.1594310828060825, 0.12056060606060606], [0.18756380534670003, 0.10188888888888888], [1.3786740554766876, 0.13272619047619047], [0.48327577174287706, 0.19721717171717168]]\n",
            "6 / 43 tests\n",
            "local_error [0.10154545 0.15430556 0.0754594  0.10100253 0.16375    0.11867424\n",
            " 0.07456818 0.08216667 0.10642857 0.12609848]\n",
            "[[0.4680755021122667, 0.10154545454545455], [3.0844718753468747, 0.15430555555555556], [0.3625878427128427, 0.0754594017094017], [0.7256881215190042, 0.10100252525252525], [0.5342930541680544, 0.16375], [0.7330535289873528, 0.11867424242424242], [1.0419160667763612, 0.07456818181818183], [0.04253883616383617, 0.08216666666666667], [0.6380350066600068, 0.10642857142857143], [2.0566341713841716, 0.12609848484848485]]\n",
            "7 / 43 tests\n",
            "local_error [0.09703205 0.16275544 0.06231546 0.10396581 0.16284965 0.14073427\n",
            " 0.11702448 0.06530769 0.12179987 0.15118298]\n",
            "[[0.3561941583098703, 0.0970320512820513], [0.6667015047216286, 0.162755439005439], [0.3595265413980584, 0.06231546231546231], [0.4880043356626178, 0.10396581196581196], [0.852215570119595, 0.16284965034965032], [0.4076687340007774, 0.14073426573426573], [0.2775844139520609, 0.11702447552447552], [0.5671524171133303, 0.0653076923076923], [1.4971399072204021, 0.1217998667998668], [0.7176448165267828, 0.1511829836829837]]\n",
            "8 / 43 tests\n",
            "local_error [0.11298138 0.16505952 0.07543956 0.10830952 0.15876984 0.10099206\n",
            " 0.07749278 0.05895238 0.09480159 0.1666342 ]\n",
            "[[0.4328177484280426, 0.11298137973137974], [0.7731514392470273, 0.16505952380952382], [0.146150308025308, 0.07543956043956043], [0.1935066167492638, 0.1083095238095238], [0.8353304497789792, 0.15876984126984128], [1.2419881311498964, 0.10099206349206348], [1.1589821975410208, 0.077492784992785], [0.697419772221243, 0.05895238095238096], [0.6869661286425993, 0.09480158730158732], [0.15522879326555794, 0.1666341991341991]]\n",
            "9 / 43 tests\n",
            "local_error [0.10444353 0.14250763 0.07782051 0.11242735 0.17334554 0.14480769\n",
            " 0.10733528 0.08312821 0.11002289 0.15692113]\n",
            "[[0.5390296990591109, 0.10444352869352869], [0.9586064507388038, 0.14250763125763125], [0.08940642200936319, 0.0778205128205128], [0.1407782478958949, 0.11242735042735044], [0.5012586408689351, 0.17334554334554333], [0.25887311136575836, 0.1448076923076923], [0.5940991720697605, 0.10733527583527584], [0.37620285841609374, 0.08312820512820512], [0.6273682094049744, 0.11002289377289377], [0.22086797597827007, 0.15692113442113442]]\n",
            "10 / 43 tests\n",
            "local_error [0.12135206 0.17565768 0.10206888 0.14955065 0.21418709 0.14945553\n",
            " 0.10251228 0.07948856 0.15112082 0.19888035]\n",
            "[[0.49842502432208297, 0.12135206133735547], [0.849839725633843, 0.17565767973856208], [0.17234612691230333, 0.1020688788335847], [0.2973170971185676, 0.14955065359477124], [0.8654862163979813, 0.21418709150326795], [1.077485547132606, 0.14945553221288516], [1.1587481619687499, 0.10251227612257025], [0.6560996225996226, 0.07948856209150326], [0.7797077350753822, 0.1511208195399372], [0.2954892697172108, 0.19888034759358286]]\n",
            "11 / 43 tests\n",
            "local_error [0.11017589 0.17255303 0.10305031 0.13875253 0.20944697 0.13315152\n",
            " 0.1149816  0.08928788 0.15088961 0.19901515]\n",
            "[[0.7440741896991895, 0.11017588783765256], [1.4471704129204128, 0.1725530303030303], [0.19058630258630255, 0.1030503108003108], [0.4416347680097681, 0.13875252525252527], [1.1775821955821955, 0.20944696969696966], [1.3547887529137532, 0.13315151515151513], [1.234719183594183, 0.11498160173160173], [0.4762055583305582, 0.08928787878787878], [0.7429400599400602, 0.15088961038961038], [0.3452601703851703, 0.19901515151515148]]\n",
            "12 / 43 tests\n",
            "local_error [0.14007055 0.15839286 0.09424908 0.13472222 0.1872381  0.17178571\n",
            " 0.13065152 0.10159524 0.15124278 0.18016595]\n",
            "[[0.4753825745977448, 0.14007054873231342], [1.0691785109884957, 0.15839285714285714], [0.48850922074141884, 0.09424908424908425], [0.5319783359914938, 0.13472222222222224], [2.718963134517701, 0.18723809523809526], [0.7831320331715067, 0.17178571428571432], [0.5244757968209982, 0.13065151515151516], [0.32007807960013834, 0.1015952380952381], [0.5633827901115055, 0.15124278499278498], [1.0504996425573052, 0.18016594516594517]]\n",
            "13 / 43 tests\n",
            "local_error [0.13542997 0.16471639 0.08147902 0.1296488  0.21441958 0.18434116\n",
            " 0.14499151 0.10884848 0.17064194 0.16382867]\n",
            "[[0.36026089605269157, 0.1354299670917318], [1.9638315627011445, 0.1647163947163947], [0.3547661412804603, 0.08147902097902097], [1.4217328056360563, 0.12964879564879567], [1.0534734884568473, 0.21441958041958042], [0.15901653669687715, 0.18434115884115884], [0.273983855970311, 0.1449915084915085], [0.11084523727906077, 0.10884848484848485], [0.35992811106162514, 0.1706419413919414], [1.565926489102573, 0.16382867132867132]]\n",
            "14 / 43 tests\n",
            "local_error [0.11910596 0.16537851 0.08796398 0.12590354 0.17906593 0.15192308\n",
            " 0.11103796 0.08913553 0.13727564 0.17961955]\n",
            "[[0.8672133169118464, 0.11910595776772247], [1.8438533272283268, 0.16537851037851037], [0.7093206817692113, 0.08796398046398046], [0.8913454503013325, 0.1259035409035409], [2.569744911950794, 0.17906593406593407], [1.4967970012014127, 0.15192307692307694], [1.8652614485188008, 0.11103796203796204], [0.4617637109622404, 0.08913553113553113], [0.6455276063805474, 0.13727564102564102], [0.883087647646471, 0.1796195471195471]]\n",
            "15 / 43 tests\n",
            "local_error [0.12006292 0.1456044  0.09329365 0.11364469 0.1839011  0.16136905\n",
            " 0.12125316 0.11189744 0.16769272 0.16666625]\n",
            "[[0.43483016330075164, 0.12006291747468217], [1.582126473200003, 0.14560439560439561], [0.4908054094598212, 0.09329365079365079], [1.5761599446305334, 0.11364468864468864], [3.2685258662905703, 0.1839010989010989], [0.6651615280144693, 0.16136904761904763], [0.9331785559212029, 0.12125316350316352], [0.11342163392163393, 0.11189743589743589], [0.5791731462981464, 0.16769272394272394], [1.328287284610814, 0.16666625041625044]]\n",
            "16 / 43 tests\n",
            "local_error [0.10388729 0.15655664 0.08891178 0.14792749 0.19901335 0.16324675\n",
            " 0.11363745 0.09670707 0.15231602 0.19088203]\n",
            "[[0.7003801582404524, 0.10388728754905224], [1.0794526233570352, 0.15655663780663778], [0.49515057573145815, 0.08891178266178265], [0.2879264159043571, 0.14792748917748916], [1.5626921754715872, 0.19901334776334778], [0.674427029669677, 0.16324675324675325], [1.1276594426815012, 0.11363744588744588], [0.7282001682957566, 0.0967070707070707], [1.0459510456863397, 0.1523160173160173], [0.6229716737510854, 0.19088203463203463]]\n",
            "17 / 43 tests\n",
            "local_error [0.13710629 0.22127564 0.10567094 0.13735043 0.1847033  0.16335281\n",
            " 0.11885964 0.08381707 0.16600125 0.18442541]\n",
            "[[0.34515311322664255, 0.13710629076805547], [0.5375717093364152, 0.22127564102564104], [0.2041824507192154, 0.10567094017094016], [0.1922802858905799, 0.13735042735042735], [1.3165933559904146, 0.1847032967032967], [0.573486081728729, 0.16335281385281383], [0.8592193019398906, 0.11885964035964036], [0.8112413477045837, 0.08381707181707182], [0.939076332980745, 0.16600124875124878], [0.5000937999908589, 0.1844254079254079]]\n",
            "18 / 43 tests\n",
            "local_error [0.11137821 0.15325794 0.0759475  0.11605556 0.13521429 0.14069048\n",
            " 0.10369913 0.06811905 0.12458333 0.15351515]\n",
            "[[0.3115621274150685, 0.11137820512820515], [0.44038305975070685, 0.1532579365079365], [0.10477103452103453, 0.07594749694749695], [0.12724856924121627, 0.11605555555555556], [2.276802595280537, 0.1352142857142857], [0.46486693127134315, 0.1406904761904762], [0.418742559727854, 0.1036991341991342], [0.841063962181609, 0.06811904761904762], [0.3912401136445252, 0.12458333333333334], [0.26524978534537347, 0.15351515151515152]]\n",
            "19 / 43 tests\n",
            "local_error [0.12278297 0.18315009 0.07928083 0.12030065 0.15063025 0.11627871\n",
            " 0.08515852 0.06402801 0.11835084 0.17234848]\n",
            "[[0.15017503330003326, 0.12278296703296704], [0.24533190502308141, 0.1831500933706816], [0.09670987590840532, 0.07928083028083029], [0.06378460836549073, 0.12030065359477124], [2.348256695102283, 0.15063025210084033], [0.7503117054514113, 0.11627871148459384], [0.5690990276063805, 0.0851585179526356], [0.9525623094226038, 0.0640280112044818], [0.4239129840747487, 0.11835084033613447], [0.08496353646353645, 0.17234848484848483]]\n",
            "20 / 43 tests\n",
            "local_error [0.09757479 0.13726496 0.05299145 0.09477778 0.16192308 0.1530641\n",
            " 0.12077972 0.08266667 0.12741117 0.14248291]\n",
            "[[0.5131662741713051, 0.09757478632478633], [2.074616801155501, 0.13726495726495724], [0.5780563910633572, 0.05299145299145299], [1.3985962281612436, 0.09477777777777778], [1.1228522143061614, 0.16192307692307695], [0.21448030985182684, 0.15306410256410255], [0.26094183834741097, 0.12077972027972027], [0.09460387325093206, 0.08266666666666665], [0.3600851876451722, 0.12741117216117218], [1.67282702892223, 0.14248290598290597]]\n",
            "21 / 43 tests\n",
            "local_error [0.12900662 0.18825779 0.11300017 0.16410332 0.22967875 0.18518335\n",
            " 0.13024586 0.11533272 0.17770722 0.22008074]\n",
            "[[0.8222674123262358, 0.12900662083015024], [1.2539623432123437, 0.18825778796367032], [0.2787579887432827, 0.11300016976487565], [0.5639348094715744, 0.16410332314744078], [1.345805384158325, 0.2296787526199291], [0.9239044166617699, 0.185183346065699], [1.156194585643115, 0.1302458619811561], [0.2937234660764072, 0.11533271793565911], [0.8326143341952165, 0.1777072192513369], [0.5580694346503171, 0.22008074441897968]]\n",
            "22 / 43 tests\n",
            "local_error [0.08260043 0.13847222 0.06444444 0.08555556 0.12911111 0.15383333\n",
            " 0.09409596 0.075      0.10328175 0.15257071]\n",
            "[[0.6462758352758351, 0.08260042735042736], [1.13399222999223, 0.13847222222222222], [0.348874542124542, 0.06444444444444444], [0.7455439837939839, 0.08555555555555557], [1.8646989676989674, 0.12911111111111112], [0.5899634254634255, 0.15383333333333332], [1.494907661782661, 0.0940959595959596], [0.23893729881229878, 0.075], [1.4782235542235542, 0.10328174603174603], [0.32956234043734034, 0.15257070707070708]]\n",
            "23 / 43 tests\n",
            "local_error [0.13837107 0.21422494 0.10118284 0.15215125 0.21037775 0.18187958\n",
            " 0.1598735  0.12053068 0.18167166 0.20517587]\n",
            "[[0.63816871561531, 0.13837106765783236], [0.9486392597770299, 0.21422494172494172], [0.3511428417643588, 0.10118284493284493], [0.5668728037957916, 0.15215125152625153], [1.6849210933545293, 0.21037774725274727], [0.6709998816113368, 0.18187957875457875], [0.5419528780043489, 0.15987350149850146], [0.4476878800164404, 0.12053067765567765], [1.0809231844007388, 0.18167166167166168], [0.9890738001248834, 0.2051758658008658]]\n",
            "24 / 43 tests\n",
            "local_error [0.11048541 0.18958983 0.07225053 0.13452201 0.20059091 0.18063312\n",
            " 0.12871753 0.0890368  0.13462662 0.18137121]\n",
            "[[0.9534522895405252, 0.11048541164717635], [0.8237864749629457, 0.18958982683982684], [0.5188946739534974, 0.07225052725052725], [0.38759984949690834, 0.1345220057720058], [2.8904257817346046, 0.2005909090909091], [1.9560164769870656, 0.18063311688311687], [0.7641523851965027, 0.12871753246753248], [0.7191159706306768, 0.08903679653679653], [1.1663959700430286, 0.13462662337662337], [0.47962875686405115, 0.18137121212121213]]\n",
            "25 / 43 tests\n",
            "local_error [0.08440079 0.16164683 0.06079365 0.11477778 0.17603175 0.11285714\n",
            " 0.09931025 0.0552619  0.11347222 0.16838023]\n",
            "[[1.2690605407664235, 0.08440079365079364], [0.3647542767689825, 0.16164682539682537], [1.197912526199291, 0.06079365079365079], [0.18585925185925187, 0.11477777777777778], [4.549327817607229, 0.176031746031746], [1.5687731835158305, 0.11285714285714285], [0.48021252930076463, 0.09931024531024532], [0.9876980992863348, 0.055261904761904755], [0.5351809277323983, 0.11347222222222221], [0.13961436602613075, 0.16838023088023085]]\n",
            "26 / 43 tests\n",
            "local_error [0.12217372 0.17236111 0.07775794 0.12099603 0.16529762 0.18571429\n",
            " 0.13974675 0.10778571 0.16140152 0.16145563]\n",
            "[[0.5482238570253276, 0.12217372333548804], [1.9896111617141028, 0.1723611111111111], [0.6247053126611948, 0.07775793650793651], [1.2333232290585234, 0.12099603174603174], [1.800499134035899, 0.16529761904761905], [0.28784984296749005, 0.18571428571428572], [0.46646985530809054, 0.13974675324675323], [0.14684589919884034, 0.1077857142857143], [0.42216955675043905, 0.16140151515151516], [1.4944446648123124, 0.16145562770562769]]\n",
            "27 / 43 tests\n",
            "local_error [0.11237069 0.18855159 0.08687823 0.12305772 0.16512987 0.11086147\n",
            " 0.10800649 0.08993362 0.16418506 0.19940043]\n",
            "[[0.7128236257206845, 0.11237069303245775], [1.2279139838919249, 0.1885515873015873], [0.5442864121172944, 0.08687823287823289], [0.31682350084555955, 0.12305772005772006], [1.8813535288241172, 0.16512987012987013], [1.3065829685020862, 0.11086147186147187], [0.8654602244161067, 0.10800649350649351], [0.7336187709349474, 0.08993362193362194], [1.0544552432861254, 0.1641850649350649], [0.4442720343382108, 0.19940043290043288]]\n",
            "28 / 43 tests\n",
            "local_error [0.0848022  0.17219933 0.06477106 0.11638889 0.16967949 0.12721612\n",
            " 0.11449767 0.05510989 0.12224634 0.1602331 ]\n",
            "[[1.624766783053547, 0.0848021978021978], [0.46904830953360377, 0.17219932844932842], [1.0146150181517828, 0.06477106227106227], [0.17935742035742033, 0.11638888888888888], [3.7644582182523347, 0.1696794871794872], [1.3695068926171865, 0.1272161172161172], [0.38768215771156944, 0.114497668997669], [0.883517051575875, 0.05510989010989011], [0.39410396955985205, 0.122246336996337], [0.22824971923501336, 0.16023310023310022]]\n",
            "29 / 43 tests\n",
            "local_error [0.09688591 0.16617854 0.05600158 0.10803175 0.15208791 0.1010111\n",
            " 0.09408095 0.05602413 0.10741677 0.16222567]\n",
            "[[1.60448074294127, 0.09688590820943763], [0.736431850485256, 0.16617853551677084], [0.7104172958534416, 0.05600158011922717], [0.30012373982226925, 0.10803174603174603], [3.205892830209007, 0.15208791208791209], [1.5450576511551746, 0.10101109674639087], [0.4923126082723606, 0.09408094846330141], [0.6685589222009501, 0.05602413273001508], [0.7586852772700294, 0.10741677440206854], [0.46997721619625643, 0.16222566649037237]]\n",
            "30 / 43 tests\n",
            "local_error [0.10630001 0.17171134 0.09206488 0.10320882 0.1760373  0.14381785\n",
            " 0.12773821 0.08976845 0.1339591  0.20210276]\n",
            "[[0.7478189310689309, 0.10630001371177843], [1.5056639887889889, 0.17171134421134418], [0.23675419025419026, 0.09206487956487956], [0.7482782634032634, 0.10320881895881896], [1.5626479353979357, 0.17603729603729604], [1.3614213702963704, 0.14381784881784884], [1.8351467143967148, 0.12773820623820625], [0.4963434898434898, 0.08976845376845377], [1.2767676073926075, 0.13395909645909643], [0.5246482683982683, 0.20210275835275837]]\n",
            "31 / 43 tests\n",
            "local_error [0.10973535 0.1064246  0.07761416 0.10098413 0.16409524 0.14383333\n",
            " 0.08284199 0.07436508 0.10677381 0.15215801]\n",
            "[[0.5149075120785647, 0.10973534798534798], [1.6068276248605196, 0.10642460317460317], [0.1341235540775014, 0.07761416361416361], [0.6044639563652722, 0.10098412698412698], [1.0067815298736351, 0.16409523809523813], [0.9957671765369134, 0.14383333333333334], [1.4341150954308843, 0.08284199134199134], [0.3199493006993007, 0.07436507936507937], [0.8626065790934213, 0.10677380952380952], [0.6192253543240385, 0.15215800865800866]]\n",
            "32 / 43 tests\n",
            "local_error [0.08962761 0.16049311 0.0596668  0.10907071 0.14317016 0.12183361\n",
            " 0.10017041 0.05541807 0.11451634 0.16317805]\n",
            "[[1.6942264824958326, 0.08962761095114036], [0.3726656656896597, 0.16049310983134515], [0.6783988298423282, 0.059666803784450835], [0.16085321867674807, 0.10907070707070708], [3.828480256671433, 0.14317016317016318], [2.1328533848332607, 0.12183360756890167], [0.489472574123503, 0.10017041455276748], [0.8545436033694546, 0.05541807212395447], [0.9704236442951616, 0.11451634150163562], [0.19924308594742032, 0.16317804744275333]]\n",
            "33 / 43 tests\n",
            "local_error [0.10695619 0.16766414 0.07775486 0.13711616 0.17784091 0.11122403\n",
            " 0.10172403 0.09335354 0.14900649 0.17532576]\n",
            "[[0.8261318101832806, 0.10695619086795558], [1.122640450236038, 0.16766414141414143], [0.4604124151991799, 0.07775485625485626], [0.6370857035121741, 0.1371161616161616], [2.2344756730197917, 0.17784090909090908], [2.018649242097772, 0.11122402597402598], [1.184877077171195, 0.10172402597402598], [0.6385228170522289, 0.09335353535353534], [1.0650302842582255, 0.1490064935064935], [0.5142151998328469, 0.1753257575757576]]\n",
            "34 / 43 tests\n",
            "local_error [0.11369789 0.1679261  0.07526668 0.11123854 0.18714286 0.12658758\n",
            " 0.08858874 0.05777356 0.10229146 0.1716342 ]\n",
            "[[0.3650496284760991, 0.11369788544788545], [0.6686494167597108, 0.16792610167610167], [0.16241466784849135, 0.07526667776667777], [0.16777325125854536, 0.11123853923853926], [1.0470269624166684, 0.18714285714285717], [0.9372241826800654, 0.1265875790875791], [1.0124059225088637, 0.08858874458874459], [0.6285806195438549, 0.057773559773559774], [0.7587368236338824, 0.10229145854145855], [0.16650281336310752, 0.17163419913419914]]\n",
            "35 / 43 tests\n",
            "local_error [0.11471687 0.16593809 0.07919775 0.12088889 0.19571429 0.13423493\n",
            " 0.10810922 0.06472361 0.12008367 0.17081002]\n",
            "[[0.41756985008455594, 0.11471686646686648], [0.6832924257115436, 0.16593808968808968], [0.11706850420821008, 0.07919774669774671], [0.11602369689134394, 0.12088888888888888], [0.7827550162256044, 0.1957142857142857], [0.5236054990760874, 0.13423493173493173], [0.45490879218820396, 0.1081092241092241], [0.8326393965511611, 0.06472360972360973], [0.4395669918316977, 0.12008366633366635], [0.17349130771189591, 0.17081002331002332]]\n",
            "36 / 43 tests\n",
            "local_error [0.11622314 0.17480922 0.07351343 0.1128547  0.17301587 0.13132784\n",
            " 0.10391342 0.0601978  0.11605769 0.17091991]\n",
            "[[0.3179885645400351, 0.11622313797313798], [0.5212042091895032, 0.17480921855921858], [0.12106020286902637, 0.07351343101343101], [0.16622751921281329, 0.11285470085470084], [0.9577022870919931, 0.17301587301587304], [0.7696820303879127, 0.13132783882783883], [0.7744995233524647, 0.1039134199134199], [0.776030627868863, 0.0601978021978022], [0.6436829608300196, 0.1160576923076923], [0.10891913315442726, 0.17091991341991342]]\n",
            "37 / 43 tests\n",
            "local_error [0.13590762 0.18628268 0.10090221 0.14352288 0.20339542 0.16560831\n",
            " 0.10579132 0.09418301 0.12549185 0.19178313]\n",
            "[[0.4220155098169807, 0.13590761689291103], [1.0113391012582191, 0.1862826797385621], [0.2023047614150555, 0.10090221216691805], [0.24737616958205194, 0.14352287581699344], [1.2096468107056344, 0.20339542483660134], [0.6179789777216249, 0.16560830999066295], [1.2971484242881304, 0.10579131652661063], [0.6272697392476805, 0.09418300653594772], [1.1188618783177608, 0.12549185128596893], [0.3965906952524599, 0.19178312537136066]]\n",
            "38 / 43 tests\n",
            "local_error [0.09804701 0.14842949 0.07517094 0.09623077 0.14931624 0.11630342\n",
            " 0.09803846 0.06928205 0.10006716 0.1610237 ]\n",
            "[[0.5089598734598735, 0.09804700854700854], [1.1069202602952606, 0.1484294871794872], [0.22322693972693966, 0.07517094017094017], [0.21088236763236765, 0.09623076923076923], [1.263986097236097, 0.14931623931623933], [0.7915721361971358, 0.11630341880341881], [1.423358682983683, 0.09803846153846153], [0.6212748501498501, 0.06928205128205128], [1.0611851620601624, 0.10006715506715506], [0.2802574231324231, 0.1610236985236985]]\n",
            "39 / 43 tests\n",
            "local_error [0.11149176 0.172529   0.07844017 0.11696032 0.14511905 0.13617216\n",
            " 0.11857459 0.05832967 0.12379579 0.15831002]\n",
            "[[0.6602527325615561, 0.11149175824175823], [0.7418580243286127, 0.17252899877899877], [0.22141987179487166, 0.07844017094017093], [0.10666204057380525, 0.11696031746031746], [2.0389486363309897, 0.14511904761904762], [0.8776167296755532, 0.13617216117216116], [0.2358049719887955, 0.11857459207459208], [0.7343675017792664, 0.05832967032967033], [0.4304395416674828, 0.12379578754578756], [0.5343641121949948, 0.1583100233100233]]\n",
            "40 / 43 tests\n",
            "local_error [0.10754487 0.13833333 0.06837607 0.097      0.18111111 0.14083333\n",
            " 0.12165152 0.08133333 0.13093254 0.1475    ]\n",
            "[[0.4089391760200582, 0.1075448717948718], [2.8026692101328323, 0.13833333333333334], [0.29471587988035347, 0.06837606837606838], [1.0530336754748515, 0.09699999999999999], [0.4853297404350039, 0.18111111111111114], [0.1374196008669693, 0.14083333333333334], [0.06611169386169385, 0.12165151515151516], [0.01966666666666667, 0.08133333333333333], [0.2488508582061213, 0.13093253968253968], [3.045232970031346, 0.1475]]\n",
            "41 / 43 tests\n",
            "local_error [0.09076565 0.15958189 0.07937174 0.1138759  0.18238672 0.13437879\n",
            " 0.10642424 0.07602309 0.11353571 0.15988095]\n",
            "[[0.7185587133454782, 0.09076565101565101], [1.033407713201831, 0.15958189033189032], [0.13201573426573426, 0.07937173937173937], [0.4166939375983493, 0.11387590187590188], [1.4012380952380952, 0.18238672438672437], [0.2948252997002997, 0.1343787878787879], [1.0561048387886616, 0.10642424242424242], [0.3153386221621515, 0.07602308802308802], [0.5425754825240119, 0.11353571428571428], [0.8103034563475738, 0.15988095238095237]]\n",
            "42 / 43 tests\n",
            "local_error [0.1041121  0.14456839 0.07675142 0.1141578  0.15786368 0.14778291\n",
            " 0.09420474 0.07529412 0.11313796 0.15479106]\n",
            "[[0.5970456740971446, 0.10411209868562811], [1.2650716464907639, 0.14456839402427638], [0.17646949945479345, 0.0767514185161244], [0.3451184966993791, 0.11415779645191408], [1.402679541863366, 0.15786367880485527], [0.42702942727207444, 0.14778291316526612], [1.4151890225787285, 0.09420473644003056], [0.5351932740462153, 0.07529411764705882], [0.8857271519330344, 0.11313795518207283], [0.4729797090164738, 0.15479106187929717]]\n",
            "43 / 43 tests\n",
            "local_error [0.09178946 0.12250541 0.05707071 0.11672583 0.12830087 0.11142677\n",
            " 0.09279942 0.07703139 0.11873196 0.15608225]\n",
            "[[1.122355282101799, 0.09178946053946055], [1.542397332328834, 0.12250541125541124], [0.4483498904346429, 0.05707070707070707], [0.46804169819688396, 0.11672582972582973], [1.7641210524082043, 0.1283008658008658], [1.3884449734561968, 0.11142676767676768], [1.066777716049001, 0.0927994227994228], [0.21853890974866197, 0.07703138528138528], [1.7178444773750505, 0.11873196248196248], [0.801988058810814, 0.15608225108225107]]\n",
            "MTR| mae:  [0.11  0.165 0.081 0.12  0.176 0.143 0.11  0.081 0.131 0.173] | ruleL: 5.0 | TIME: 0.5656106527461562 | Coverage: 0.022714981070849124 | avg estimators: 95.488 / 100\n",
            " GS| mae:  [0.161 0.193 0.083 0.153 0.416 0.259 0.171 0.115 0.204 0.176] | ruleL: 3.7674418604651163 | TIME: 0.0007285129192263581 | Coverage: 0.09572742022714979\n",
            " LS| mae:  [0.126 0.198 0.073 0.138 0.23  0.249 0.129 0.078 0.151 0.137] | ruleL: 2.558139534883721 | TIME: 2.742675831151563 | Coverage: 0.14169821525148735\n",
            "MAR| mae:  [0.096 0.177 0.08  0.116 0.181 0.16  0.132 0.063 0.155 0.128] | ruleL: 4.093023255813954 | TIME: 0.27466442973114724 | Coverage: 0.028664142779881042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# facebook"
      ],
      "metadata": {
        "id": "pMpd5Sw9st7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############\n",
        "# facebook 500x18, 4 targets\n",
        "############\n",
        "fb_df = pd.read_csv('dataset_Facebook.csv', sep=';', nrows=200)\n",
        "\n",
        "# fill NaN\n",
        "fb_df['like'].fillna(0,inplace=True)\n",
        "fb_df['share'].fillna(0,inplace=True)\n",
        "fb_df['Paid'].fillna(0,inplace=True)\n",
        "fb_df.drop(['Type'], inplace=True, axis=1)\n",
        "\n",
        "\n",
        "# get column names\n",
        "column_names = fb_df.columns\n",
        "\n",
        "# get data/target names\n",
        "f_n = column_names[:14]\n",
        "t_n = ['comment', 'share']\n",
        "\n",
        "X = fb_df[f_n]\n",
        "y = fb_df[t_n]\n",
        "\n",
        "# convert to numpy\n",
        "X = X.to_numpy()\n",
        "y = y.to_numpy()\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "counter = 1\n",
        "for train_index, test_index in kf.split(X):\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "  print(\"iteration:\", counter)\n",
        "  counter += 1\n",
        "  doTest(X_train, X_test, y_train, y_test, f_n, t_n, 0.9)"
      ],
      "metadata": {
        "id": "FZZIPiyIZAu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28dd5350-86c3-4a85-839b-b5259c38f7d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration: 1\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [2.335 4.724] | TIME: 0.148 | Coverage: 0.05000000000000001 | avg estimators: 92.4 / 100\n",
            " GS| mae:  [6.735 9.078] | TIME: 0.001 | Coverage: 0.11499999999999996\n",
            " LS| mae:  [6.242 8.927] | TIME: 3.178 | Coverage: 0.18749999999999997\n",
            "iteration: 2\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [1.796 4.197] | TIME: 0.14 | Coverage: 0.19249999999999995 | avg estimators: 78.85 / 100\n",
            " GS| mae:  [ 4.185 10.995] | TIME: 0.001 | Coverage: 0.10999999999999999\n",
            " LS| mae:  [ 3.666 10.615] | TIME: 3.103 | Coverage: 0.18750000000000003\n",
            "iteration: 3\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [0.625 1.415] | TIME: 0.136 | Coverage: 0.095 | avg estimators: 93.05 / 100\n",
            " GS| mae:  [ 2.823 15.391] | TIME: 0.001 | Coverage: 0.10500000000000001\n",
            " LS| mae:  [ 3.09  16.705] | TIME: 3.111 | Coverage: 0.15749999999999997\n",
            "iteration: 4\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [1.561 3.209] | TIME: 0.146 | Coverage: 0.1424999999999999 | avg estimators: 84.9 / 100\n",
            " GS| mae:  [ 2.335 10.85 ] | TIME: 0.001 | Coverage: 0.095\n",
            " LS| mae:  [ 2.41  10.248] | TIME: 3.184 | Coverage: 0.18\n",
            "iteration: 5\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [1.167 2.72 ] | TIME: 0.144 | Coverage: 0.09750000000000002 | avg estimators: 91.3 / 100\n",
            " GS| mae:  [3.997 8.324] | TIME: 0.001 | Coverage: 0.10500000000000001\n",
            " LS| mae:  [4.209 8.11 ] | TIME: 3.061 | Coverage: 0.18249999999999997\n",
            "iteration: 6\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [3.608 8.84 ] | TIME: 0.137 | Coverage: 0.095 | avg estimators: 87.9 / 100\n",
            " GS| mae:  [ 7.978 16.958] | TIME: 0.001 | Coverage: 0.08000000000000003\n",
            " LS| mae:  [ 9.646 21.556] | TIME: 3.122 | Coverage: 0.14\n",
            "iteration: 7\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [ 4.959 14.822] | TIME: 0.143 | Coverage: 0.14499999999999993 | avg estimators: 72.25 / 100\n",
            " GS| mae:  [13.414 21.661] | TIME: 0.001 | Coverage: 0.08500000000000003\n",
            " LS| mae:  [13.718 22.331] | TIME: 3.128 | Coverage: 0.12749999999999997\n",
            "iteration: 8\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [1.942 3.889] | TIME: 0.147 | Coverage: 0.047500000000000014 | avg estimators: 94.2 / 100\n",
            " GS| mae:  [ 5.671 12.979] | TIME: 0.001 | Coverage: 0.10499999999999998\n",
            " LS| mae:  [ 5.955 14.829] | TIME: 3.108 | Coverage: 0.17250000000000001\n",
            "iteration: 9\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [0.664 1.247] | TIME: 0.142 | Coverage: 0.14499999999999993 | avg estimators: 87.95 / 100\n",
            " GS| mae:  [3.483 9.693] | TIME: 0.001 | Coverage: 0.10500000000000005\n",
            " LS| mae:  [ 3.063 10.201] | TIME: 3.089 | Coverage: 0.20249999999999999\n",
            "iteration: 10\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [0.539 1.116] | TIME: 0.143 | Coverage: 0.04500000000000001 | avg estimators: 98.2 / 100\n",
            " GS| mae:  [ 5.599 11.927] | TIME: 0.001 | Coverage: 0.09\n",
            " LS| mae:  [ 5.862 13.341] | TIME: 3.117 | Coverage: 0.1925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# River flow"
      ],
      "metadata": {
        "id": "l-74I50D6M-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# 9125x576 + 8\n",
        "RF2_df = pd.read_csv('RF2.csv')\n",
        "RF2_df = RF2_df.dropna(axis=0)\n",
        "\n",
        "RF2_cols = RF2_df.columns\n",
        "RF2_df = RF2_df[RF2_cols[1:]].reset_index(drop=True)\n",
        "RF2_df = RF2_df.iloc[:200]\n",
        "\n",
        "RF2_cols = RF2_df.columns\n",
        "f_n = RF2_cols[:576]\n",
        "t_n = RF2_cols[576:]\n",
        "f_n = f_n[:15]\n",
        "t_n = t_n[:5]\n",
        "\n",
        "X = RF2_df[f_n]\n",
        "y = RF2_df[t_n]\n",
        "\n",
        "# convert to numpy\n",
        "X = X.to_numpy()\n",
        "y = y.to_numpy()\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "counter = 1\n",
        "for train_index, test_index in kf.split(X):\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "  print(\"iteration:\", counter)\n",
        "  counter += 1\n",
        "  doTest(X_train, X_test, y_train, y_test, f_n, t_n, 0.08)"
      ],
      "metadata": {
        "id": "CcdQaKQX9LGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMs3HmOXCWF_",
        "outputId": "6de9ff06-fc92-4b6c-998c-005a8147a9fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration: 1\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [1.698 0.003 1.627 0.901 0.66 ] | TIME: 0.139 | Coverage: 0.05000000000000001 | avg estimators: 95.55 / 100\n",
            " GS| mae:  [1.273 0.017 1.351 0.616 1.223] | TIME: 0.001 | Coverage: 0.09500000000000001\n",
            " LS| mae:  [1.176 0.016 1.078 0.678 1.194] | TIME: 2.493 | Coverage: 0.10500000000000001\n",
            "iteration: 2\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [0.866 0.001 0.871 0.456 0.302] | TIME: 0.14 | Coverage: 0.05000000000000001 | avg estimators: 97.75 / 100\n",
            " GS| mae:  [2.515 0.016 2.072 0.819 1.64 ] | TIME: 0.001 | Coverage: 0.11499999999999996\n",
            " LS| mae:  [1.628 0.015 1.615 0.614 1.026] | TIME: 2.53 | Coverage: 0.07250000000000002\n",
            "iteration: 3\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [1.292 0.002 1.172 0.673 0.567] | TIME: 0.149 | Coverage: 0.05000000000000001 | avg estimators: 96.5 / 100\n",
            " GS| mae:  [1.44  0.015 1.881 0.963 2.063] | TIME: 0.001 | Coverage: 0.08499999999999999\n",
            " LS| mae:  [1.141 0.012 2.053 0.752 1.078] | TIME: 2.669 | Coverage: 0.13499999999999995\n",
            "iteration: 4\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [1.761 0.003 1.766 0.911 0.625] | TIME: 0.142 | Coverage: 0.04500000000000001 | avg estimators: 95.4 / 100\n",
            " GS| mae:  [1.509 0.014 2.134 0.902 1.43 ] | TIME: 0.001 | Coverage: 0.10999999999999999\n",
            " LS| mae:  [1.753 0.011 1.74  1.252 1.382] | TIME: 2.462 | Coverage: 0.08000000000000004\n",
            "iteration: 5\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [1.278 0.002 1.207 0.66  0.448] | TIME: 0.141 | Coverage: 0.04250000000000001 | avg estimators: 96.7 / 100\n",
            " GS| mae:  [1.342 0.016 1.454 1.086 0.852] | TIME: 0.001 | Coverage: 0.11500000000000002\n",
            " LS| mae:  [1.054 0.016 1.456 0.778 0.712] | TIME: 2.456 | Coverage: 0.11750000000000002\n",
            "iteration: 6\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [2.376 0.003 2.249 1.204 0.883] | TIME: 0.143 | Coverage: 0.047500000000000014 | avg estimators: 93.9 / 100\n",
            " GS| mae:  [2.51  0.019 3.092 1.639 1.915] | TIME: 0.001 | Coverage: 0.18500000000000003\n",
            " LS| mae:  [1.053 0.016 1.066 0.513 0.709] | TIME: 2.534 | Coverage: 0.1\n",
            "iteration: 7\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [2.939 0.005 2.767 1.52  1.291] | TIME: 0.146 | Coverage: 0.05000000000000001 | avg estimators: 92.05 / 100\n",
            " GS| mae:  [0.866 0.017 1.599 0.722 1.381] | TIME: 0.001 | Coverage: 0.13499999999999998\n",
            " LS| mae:  [1.023 0.016 1.639 0.654 0.725] | TIME: 2.487 | Coverage: 0.08750000000000001\n",
            "iteration: 8\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [0.927 0.001 0.802 0.471 0.361] | TIME: 0.138 | Coverage: 0.05000000000000001 | avg estimators: 97.65 / 100\n",
            " GS| mae:  [1.486 0.018 2.686 1.072 1.272] | TIME: 0.001 | Coverage: 0.14000000000000007\n",
            " LS| mae:  [1.021 0.013 1.713 0.521 0.932] | TIME: 2.543 | Coverage: 0.08250000000000002\n",
            "iteration: 9\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [2.142 0.004 2.155 1.151 0.777] | TIME: 0.144 | Coverage: 0.05000000000000001 | avg estimators: 94.3 / 100\n",
            " GS| mae:  [1.662 0.019 2.142 0.628 1.225] | TIME: 0.0 | Coverage: 0.08000000000000004\n",
            " LS| mae:  [1.399 0.017 2.021 0.797 1.1  ] | TIME: 2.471 | Coverage: 0.1475\n",
            "iteration: 10\n",
            "training MTR\n",
            "training GS\n",
            "training LS\n",
            "0 / 20 tests\n",
            "1 / 20 tests\n",
            "2 / 20 tests\n",
            "3 / 20 tests\n",
            "4 / 20 tests\n",
            "5 / 20 tests\n",
            "6 / 20 tests\n",
            "7 / 20 tests\n",
            "8 / 20 tests\n",
            "9 / 20 tests\n",
            "10 / 20 tests\n",
            "11 / 20 tests\n",
            "12 / 20 tests\n",
            "13 / 20 tests\n",
            "14 / 20 tests\n",
            "15 / 20 tests\n",
            "16 / 20 tests\n",
            "17 / 20 tests\n",
            "18 / 20 tests\n",
            "19 / 20 tests\n",
            "MTR| mae:  [0.878 0.002 0.857 0.474 0.615] | TIME: 0.143 | Coverage: 0.047500000000000014 | avg estimators: 97.15 / 100\n",
            " GS| mae:  [1.377 0.013 1.383 0.87  0.998] | TIME: 0.001 | Coverage: 0.07000000000000003\n",
            " LS| mae:  [1.557 0.014 1.566 0.825 1.511] | TIME: 2.431 | Coverage: 0.12000000000000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AVG OF K-FOLD\n",
        "#https://github.com/tsoumakas/mulan/blob/master/data/multi-target/andro.arff\n",
        "#https://github.com/tsoumakas/mulan/blob/master/data/multi-target/wq.arff\n",
        "#https://github.com/tsoumakas/mulan/blob/master/data/multi-target/osales.arff"
      ],
      "metadata": {
        "id": "A3BmNWAaNGY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from algorithms.MARLENA.marlena.marlena.marlena import MARLENA\n",
        "# load data\n",
        "slump_data = arff.loadarff('slump.arff')\n",
        "slump_df = pd.DataFrame(slump_data[0])\n",
        "\n",
        "# get column names\n",
        "column_names = slump_df.columns\n",
        "\n",
        "# get data/target names\n",
        "f_n = column_names[:7]\n",
        "t_n = column_names[7:]\n",
        "\n",
        "X = slump_df[f_n]\n",
        "y = slump_df[t_n]\n",
        "\n",
        "# convert to numpy\n",
        "X = X.to_numpy()\n",
        "y = y.to_numpy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "# Marlena fidelity to mae at the end\n",
        "MTR_obj = MTR(model=None, X_train = X_train, X_test=X_test, y_train=y_train, y_test=y_test, feature_names=f_n, target_names=t_n)\n",
        "model = MTR_obj.getModel()\n",
        "#predictions = model.predict(X_train)\n",
        "\n",
        "instance = X_test[0]\n",
        "feature_names = f_n\n",
        "label_names = t_n\n",
        "\n",
        "m1 = MARLENA(neigh_type='mixed', random_state=42)\n",
        "i2e = pd.Series(instance, index=feature_names)\n",
        "X2E = pd.DataFrame(X_train, columns=feature_names)\n",
        "# returns rule, mask, list_split_conditions, len_rule, instance_imporant_feat, fidelity, hit, DT\n",
        "_, MarlenaPreds, list_split_conditions, len_rule, _, _, _, _ = m1.extract_explanation(i2e, X2E, model, feature_names, [],\n",
        "                                          label_names, k=10, size=50, alpha=0.7)"
      ],
      "metadata": {
        "id": "CJb_u5LSSKBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test values"
      ],
      "metadata": {
        "id": "u0DP19b6XLot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actualpreds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G55_KYJgxRk3",
        "outputId": "394fca4f-b055-42ca-98ce-eb8a4495b421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[24.  , 60.  , 45.82],\n",
              "       [24.  , 60.  , 45.82],\n",
              "       [24.  , 60.  , 45.82],\n",
              "       [24.  , 60.  , 45.82]])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MTRpreds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrHkr8zvteK_",
        "outputId": "1491f2ab-5dd2-4491-ab72-755cf4143519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[13.5575,  2.685 ],\n",
              "        [37.44  ,  4.645 ],\n",
              "        [45.8797,  3.5429]],\n",
              "\n",
              "       [[13.5575,  2.685 ],\n",
              "        [37.44  ,  4.645 ],\n",
              "        [45.8797,  3.5429]],\n",
              "\n",
              "       [[13.5575,  2.685 ],\n",
              "        [37.44  ,  4.645 ],\n",
              "        [45.8797,  3.5429]],\n",
              "\n",
              "       [[13.5575,  2.685 ],\n",
              "        [37.44  ,  4.645 ],\n",
              "        [45.8797,  3.5429]]])"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_array = np.array([subarray[:,1] for subarray in MTRpreds])\n",
        "column_averages = np.mean(new_array, axis=0)\n",
        "column_averages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmbzBmHxtW3E",
        "outputId": "669f728c-0bfc-4f07-870f-0da157c34d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.685 , 4.645 , 3.5429])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(actualpreds[0])\n",
        "print(MTRpreds[0])\n",
        "print(GSpreds[0])\n",
        "print(LSpreds[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi_KoanApL7T",
        "outputId": "e5f427cc-023d-4eb2-8d44-1d97845060c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[24.   60.   45.82]\n",
            "[[13.5575  2.685 ]\n",
            " [37.44    4.645 ]\n",
            " [45.8797  3.5429]]\n",
            "[16.68    42.815   38.35685]\n",
            "[12.38875 33.85    52.72455]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(calc_al_error(y_test[10], 0.1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtiMJcbihbGF",
        "outputId": "1a9476d4-3c7a-41cd-a1fb-3ac826ea4e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.0500000000000003, 4.82, 4.93]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# experiments\n",
        "# mae, len, cov, time\n",
        "# k-fold\n",
        "# allowed_error []\n",
        "#https://github.com/intelligence-csd-auth-gr/LionLearn/blob/master/LionForests_Multi/experiments/C3.%20WaterQuality.ipynb"
      ],
      "metadata": {
        "id": "gubJb4qCE7mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OLD"
      ],
      "metadata": {
        "id": "Zno7j6fmBz5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "using MTR"
      ],
      "metadata": {
        "id": "rq63E5SmP6Hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MTR_obj = MTR(model=None, X_train = X_train, X_test=X_test, y_train=y_train, y_test=y_test, feature_names=f_n, target_names=t_n)\n",
        "rule = MTR_obj.explain(instance, 5) # you can add as last arguement the allowed error\n",
        "featureLimits = MTR_obj.getFeatureLimits()\n",
        "decisionsAndErrors = MTR_obj.getDecisionsAndErros()\n",
        "print(decisionsAndErrors)\n",
        "print(rule)\n",
        "\n",
        "# this model will be used for L/G surrogate\n",
        "model = MTR_obj.getModel()\n",
        "predictions = model.predict(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo-D_hAfP8PM",
        "outputId": "b17c0c1d-2f1c-46ed-d9dc-ff9699c2e733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "allowed_error [5 5 5]\n",
            "reduced_rules:  89 / 100\n",
            "[[13.5575, 2.685], [37.44, 4.645], [45.8797, 3.5429]]\n",
            "if 167.0<=Water<=168.5 & 904.0<=Coarse_Aggr<=917.5 & 0.0<=Slag<=0.05 & 801.5<=Fine_Aggr<=805.0 & 309.5<=Cemment<=310.0 & 142.5<=Fly_ash<=143.0 & 9.5<=SP<=10.0 then SLUMP_cm: 13.5575 +/- 2.685 error, FLOW_cm: 37.44 +/- 4.645 error, Compressive_Strength_Mpa: 45.8797 +/- 3.5429 error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using global surrogate"
      ],
      "metadata": {
        "id": "L-lG18OuP953"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GS = GlobalSurrogateTree(X_train, predictions, f_n)\n",
        "r, GSp = GS.rule(instance)\n",
        "print(GS.rule(instance))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaKyvBdFQBnt",
        "outputId": "978a4e42-0263-4469-da6f-991bab64607a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'Water': [['<=', 182.25]], 'Slag': [['<=', 66.39999961853027]], 'Coarse_Aggr': [['<=', 1048.2999877929688], ['>', 904.0]], 'Fly_ash': [['<=', 210.9499969482422]]}, array([16.68   , 42.815  , 38.35685]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using local surrogate"
      ],
      "metadata": {
        "id": "Arqz8_w-QCtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LS = LocalSurrogateTree(X_train, predictions, f_n, 40) # neigns should be >= 10\n",
        "rl, LSp = LS.rule(instance)\n",
        "print(LS.rule(instance))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKoGZqfrQDCy",
        "outputId": "475fe8c0-6853-4196-8bdc-0bc6e47c4151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'Fly_ash': [['<=', 210.7123150630344], ['>', 34.602104331589345]], 'Water': [['<=', 181.24161005543618]], 'Cemment': [['>', 298.04265509501226]]}, array([12.38875, 33.85   , 52.72455]))\n"
          ]
        }
      ]
    }
  ]
}